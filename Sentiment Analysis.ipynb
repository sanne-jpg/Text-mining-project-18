{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75305eb1",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "First we'll start with processing the data and installing all needed modules so that sentiment analysis can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26eeaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting NRCLex\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Users/clairek/opt/anaconda3/lib/python3.9/site-packages (from textblob->NRCLex) (3.6.5)\n",
      "Requirement already satisfied: click in /Users/clairek/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob->NRCLex) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/clairek/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob->NRCLex) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/clairek/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob->NRCLex) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /Users/clairek/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob->NRCLex) (4.62.3)\n",
      "Building wheels for collected packages: NRCLex\n",
      "  Building wheel for NRCLex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43328 sha256=1c197fccb3629572ea1c24d9e3c34ee4e7db73c132fae77f0fe68b15a9994269\n",
      "  Stored in directory: /Users/clairek/Library/Caches/pip/wheels/68/c4/f2/c390dd3eac398fdf45f7a01c6516bc53fa7a9ab59c7d2ff518\n",
      "Successfully built NRCLex\n",
      "Installing collected packages: textblob, NRCLex\n",
      "Successfully installed NRCLex-3.0.0 textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0661be3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/clairek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Import required modules\n",
    "from nrclex import NRCLex\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "my_stop_words = STOPWORDS.union(set(['yeah', 'cause', 'wanna', 'gonna', 'nigga', 'fuckin', 'bitch', 'come'])) #extra stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ea05ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/xb1nbzgn01z2vz9b_dmxkcsh0000gn/T/ipykernel_20924/1435521432.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>New Rules</td>\n",
       "      <td>one one one one one   talkin' in my sleep at n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Don’t Start Now</td>\n",
       "      <td>if you don't wanna see me   did a full 80 craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>IDGAF</td>\n",
       "      <td>you call me all friendly tellin' me how much y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Blow Your Mind (Mwah)</td>\n",
       "      <td>i know it's hot i know we've got something tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Be the One</td>\n",
       "      <td>i see the moon i see the moon i see the moon o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>Khalid</td>\n",
       "      <td>Khalid - Vertigo  (Tradução Português)</td>\n",
       "      <td>será que é melhor apenas acreditar nas teorias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>Khalid</td>\n",
       "      <td>Better (Miles Away Remix)</td>\n",
       "      <td>i'm not really drunk i never get that fucked u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>Khalid</td>\n",
       "      <td>Khalid - Better (Official Music Video)</td>\n",
       "      <td>users considering it's a virus or malware must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403</th>\n",
       "      <td>Khalid</td>\n",
       "      <td>Perfect Lover</td>\n",
       "      <td>lyrics for this song have yet to be released p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>Khalid</td>\n",
       "      <td>Better (Rennie! Remix)</td>\n",
       "      <td>love to see you shine in the night like the di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5378 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Artist                                   Title  \\\n",
       "0     Dua Lipa                               New Rules   \n",
       "1     Dua Lipa                         Don’t Start Now   \n",
       "2     Dua Lipa                                   IDGAF   \n",
       "3     Dua Lipa                   Blow Your Mind (Mwah)   \n",
       "4     Dua Lipa                              Be the One   \n",
       "...        ...                                     ...   \n",
       "5400    Khalid  Khalid - Vertigo  (Tradução Português)   \n",
       "5401    Khalid               Better (Miles Away Remix)   \n",
       "5402    Khalid  Khalid - Better (Official Music Video)   \n",
       "5403    Khalid                           Perfect Lover   \n",
       "5404    Khalid                  Better (Rennie! Remix)   \n",
       "\n",
       "                                                  Lyric  \n",
       "0     one one one one one   talkin' in my sleep at n...  \n",
       "1     if you don't wanna see me   did a full 80 craz...  \n",
       "2     you call me all friendly tellin' me how much y...  \n",
       "3     i know it's hot i know we've got something tha...  \n",
       "4     i see the moon i see the moon i see the moon o...  \n",
       "...                                                 ...  \n",
       "5400  será que é melhor apenas acreditar nas teorias...  \n",
       "5401  i'm not really drunk i never get that fucked u...  \n",
       "5402  users considering it's a virus or malware must...  \n",
       "5403  lyrics for this song have yet to be released p...  \n",
       "5404  love to see you shine in the night like the di...  \n",
       "\n",
       "[5378 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string \n",
    "import operator\n",
    "\n",
    "### Load dataset, adapt the path to point to your local copy of the dataset\n",
    "path = \"/Users/clairek/Desktop/Year 3/ba-text-mining-master/project/archive/song_info_csv/\"\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "### Instert dataset into dataframe\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)  \n",
    "\n",
    "### Take out only the lyrics from the full dataframe\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "data_text = df[['Lyric']]\n",
    "\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "### Take out any empty lyrics rows \n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "df_new = df[df['Lyric'].notnull()]\n",
    "df_new.loc[:, ~df_new.columns.isin(['Date', 'Year', 'Unnamed: 0', 'Album'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3703e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in my_stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae844437",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a list from the dataframe colummn\n",
    "lyrics_list = df_new['Lyric'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48eb4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotions(text, dataframe):\n",
    "    \"\"\"\n",
    "    Function that will take a text at return a dataframe of emotions (excluding positive and negative)\n",
    "    Params:\n",
    "        text (str): The song of interest\n",
    "        dataframe (Pandas DataFrame): The emotions dataframe to add values\n",
    "        \n",
    "    Return:\n",
    "        Updated dataframe with the new row of interest\n",
    "    \"\"\"\n",
    "    nrc_text = NRCLex(text)\n",
    "    nrc_dict = nrc_text.affect_frequencies\n",
    "    nrc_dict.pop('positive', None)\n",
    "    nrc_dict.pop('negative', None)\n",
    "    nrc_dict.pop('anticip', None)\n",
    "    nrc_max_emotion = max(nrc_dict.items(), key=operator.itemgetter(1))[0]\n",
    "    nrc_dict[\"max_emotion\"] = nrc_max_emotion\n",
    "    dataframe = dataframe.append(nrc_dict, ignore_index= True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d42caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a dataframe of the different emotions\n",
    "emotion_df = pd.DataFrame(columns = ['fear', 'anger', 'trust', 'surprise', 'sadness', 'disgust', 'joy', 'anticipation', 'max_emotion'])\n",
    "\n",
    "### Run lyrics through NRC to assign emotions\n",
    "for text in lyrics_list:\n",
    "    emotion_df = get_emotions(text, emotion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b64fb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a list of all the emotions\n",
    "label_gold = []\n",
    "\n",
    "for emotion in emotion_df['max_emotion']:\n",
    "    label_gold.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99fcfd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clairek/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.02      1.00      0.03         2\n",
      "anticipation       0.01      1.00      0.02         1\n",
      "     disgust       0.00      0.00      0.00         0\n",
      "        fear       0.36      0.75      0.48       114\n",
      "         joy       0.99      0.31      0.48       954\n",
      "     sadness       0.05      1.00      0.09         5\n",
      "    surprise       0.00      0.00      0.00         0\n",
      "       trust       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.36      1076\n",
      "   macro avg       0.18      0.51      0.14      1076\n",
      "weighted avg       0.92      0.36      0.47      1076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clairek/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/clairek/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/clairek/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### Initialize lyric object, and then turn lyric train data into a vector \n",
    "lyric_vec = CountVectorizer(min_df=0, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "lyric_counts = lyric_vec.fit_transform(lyrics_list)\n",
    "\n",
    "### Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "song_tfidf = tfidf_transformer.fit_transform(lyric_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    song_tfidf, # the tf-idf model\n",
    "    label_gold, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "### Train a multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "\n",
    "### Predicting the Test set results, find macro recall\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "sklearn.metrics.recall_score(y_true=y_test,\n",
    "                             y_pred=y_pred,\n",
    "                             average='macro') \n",
    "\n",
    "print(classification_report(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
